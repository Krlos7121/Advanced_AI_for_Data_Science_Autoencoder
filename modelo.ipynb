{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2c8cbc",
   "metadata": {},
   "source": [
    "# Conversión de imagenes en escala de grises a color usando TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7fc8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-metal (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-metal\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/krlos/miniconda3/lib/python3.13/site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krlos/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Users/krlos/miniconda3/lib/python3.13/site-packages (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-metal\n",
    "%pip install matplotlib\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Flatten, Dense, Reshape, MaxPool2D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1471b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos físicos disponibles: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dispositivos físicos disponibles:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81807268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar conjuntos de imágenes (no es necesario aplicar transformaciones)\n",
    "COLOR_PATH = 'color_processed/'\n",
    "GRAY_PATH = 'gray_processed/'\n",
    "\n",
    "def load_image_pair(): \n",
    "    gray_images = []\n",
    "    color_images = []\n",
    "    for img in os.listdir(GRAY_PATH): #Mismo número de imágenes en ambas carpetas\n",
    "        image_gray = tf.io.read_file(os.path.join(GRAY_PATH + img)) #Cargar cada imagen\n",
    "        image_color = tf.io.read_file(os.path.join(COLOR_PATH + img))\n",
    "        \n",
    "        gray_img_tensor_int = tf.image.decode_jpeg(image_gray, channels=1)\n",
    "        color_img_tensor_int = tf.image.decode_jpeg(image_color, channels=3)\n",
    "    \n",
    "        gray_img_float = tf.image.convert_image_dtype(gray_img_tensor_int, tf.float32, )\n",
    "        color_img_float = tf.image.convert_image_dtype(color_img_tensor_int, tf.float32)    \n",
    "        \n",
    "        gray_images.append(gray_img_float)\n",
    "        color_images.append(color_img_float)\n",
    "\n",
    "    return gray_images, color_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_image_pair()\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Luego separar train de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c89c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64) \n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "LATENT_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del Modelo Autoencoder\n",
    "encoder = Sequential([\n",
    "    Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1)),\n",
    "\n",
    "    Conv2D(32, (3,3), strides=1, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(64, (3,3), strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, (3,3), strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3,3), strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(LATENT_DIM, activation='relu'),  # latent_dim\n",
    "    \n",
    "    Dropout(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d01a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([\n",
    "    Input(shape=((LATENT_DIM,))),\n",
    "    Dense(8*8*256, activation='relu'),\n",
    "    Reshape((8, 8, 256)),\n",
    "\n",
    "    Conv2DTranspose(128, (3,3), strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2DTranspose(64, (3,3), strides=2, padding='same', activation='relu'), \n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2DTranspose(32, (3,3), strides=2, padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(3, (3,3), padding='same', activation='sigmoid')  # salida [0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68369e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,487,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,502,595</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m2,487,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │     \u001b[38;5;34m2,502,595\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,989,635</span> (19.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,989,635\u001b[0m (19.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,988,227</span> (19.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,988,227\u001b[0m (19.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([encoder, decoder])\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fd62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_autoencoder_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=1e-3)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd3c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 526ms/step - loss: 0.2002 - val_loss: 0.2042 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 512ms/step - loss: 0.1543 - val_loss: 0.2164 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 516ms/step - loss: 0.1480 - val_loss: 0.2125 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 515ms/step - loss: 0.1448 - val_loss: 0.2257 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 515ms/step - loss: 0.1426 - val_loss: 0.2164 - learning_rate: 5.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 514ms/step - loss: 0.1408 - val_loss: 0.2106 - learning_rate: 5.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 514ms/step - loss: 0.1400 - val_loss: 0.2098 - learning_rate: 5.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 518ms/step - loss: 0.1390 - val_loss: 0.1955 - learning_rate: 2.5000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 517ms/step - loss: 0.1382 - val_loss: 0.1855 - learning_rate: 2.5000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 520ms/step - loss: 0.1377 - val_loss: 0.1817 - learning_rate: 2.5000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 537ms/step - loss: 0.1372 - val_loss: 0.1686 - learning_rate: 2.5000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 545ms/step - loss: 0.1369 - val_loss: 0.1588 - learning_rate: 2.5000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 533ms/step - loss: 0.1366 - val_loss: 0.1551 - learning_rate: 2.5000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 530ms/step - loss: 0.1363 - val_loss: 0.1450 - learning_rate: 2.5000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 540ms/step - loss: 0.1363 - val_loss: 0.1382 - learning_rate: 2.5000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 531ms/step - loss: 0.1356 - val_loss: 0.1362 - learning_rate: 2.5000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 544ms/step - loss: 0.1351 - val_loss: 0.1335 - learning_rate: 2.5000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 542ms/step - loss: 0.1350 - val_loss: 0.1368 - learning_rate: 2.5000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 543ms/step - loss: 0.1347 - val_loss: 0.1329 - learning_rate: 2.5000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 540ms/step - loss: 0.1340 - val_loss: 0.1329 - learning_rate: 2.5000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 543ms/step - loss: 0.1337 - val_loss: 0.1336 - learning_rate: 2.5000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 540ms/step - loss: 0.1338 - val_loss: 0.1380 - learning_rate: 2.5000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 557ms/step - loss: 0.1330 - val_loss: 0.1327 - learning_rate: 1.2500e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 548ms/step - loss: 0.1328 - val_loss: 0.1349 - learning_rate: 1.2500e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 539ms/step - loss: 0.1323 - val_loss: 0.1357 - learning_rate: 1.2500e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 541ms/step - loss: 0.1322 - val_loss: 0.1347 - learning_rate: 1.2500e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 539ms/step - loss: 0.1318 - val_loss: 0.1352 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1662842f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), \n",
    "          validation_data=(np.array(X_val), np.array(y_val)),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
